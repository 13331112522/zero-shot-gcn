# Zero-shot GCN

An implementation of **Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs**.
In the paper, we propose to utilize both semantic embedding and class relation graph to
learn visual classifiers for zero-shot classes (categoryies that do not appear in the CNN training process).

![](data/docs/git-gcn-teaser.png)
The pipeline is as the figure above depicts. It consists of two network: CNN and GCN module.
We take CNN as off-the-shelf network (ImageNet-1k pretrain specifically) to extract image feature and provide its fc7 classifier as ground truth for the GCN. The GCN module is trained to embed every class node to the space of classifier weight by applying the loss to only seen classes.
During the inference, the image feature is extracted by the CNN network which has never seen this class in training time.
The feature then dot products the classifier which are generated by GCN module. The class with maximum score is selected as prediction. For more details, please see the [paper](https://arxiv.org/abs/1803.08035).
(The implementation of graph convolutional layer is based on [this repo](https://github.com/tkipf/gcn/tree/master/gcn).)


**Note**:
- We provide a CNN feature extractor in tensorflow. However, in the original paper, the CNN part is in Caffe. The result might vary within $1\%$.


## Installation

```bash
git clone git@github.com:JudyYe/zero-shot-gcn.git
cd zero-shot-gcn/src
```

## Prepare data
The total images will take about 1.1T. The extracted feature will also be over 30G. It is recommended to save them to your large space storage $IMAGE_FOLDER $FEAT_FOLDER and soft link them.
```
cd ..
ln -s $IMAGE_FOLDER images
ln -s $FEAT_FOLDER feats
cd src
```

### Download images of unseen class from ImageNet.

To get access to original images from ImageNet, you need to [register](http://image-net.org/signup) a pair of username and access key.
```
python tools/download_image.py --user $YOUR_USER_NAME --key $YOUR_ACCESS_KEY --save_dir ../images/ --hop 2 &
```

**Note**:
- Downloading images of all unseen classes will take up to one day. We only download the 2-hops subset. You may choose to download 3-hops subset of all unseen classes by settting --hop to 3 or all.
- The script supports multi-process. It is recommended to run several copies.

### Extract visual features of images
1. **Download** ImageNet preatrained CNN (resnet 50)
```
mkdir ../pretrain_weights && cd ../pretrain_weights
wget http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz
tar xf resnet_v1_50_2016_08_28.tar.gz
cd ../src
```
To download  Inception pretrained model:
```
cd ../pretrain_weights
wget http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz
tar xf inception_v1_2016_08_28.tar.gz
cd ../src
```
2. **Extract feature**. The following will give you default mode: extracting the feature with pretrained `res50` for `2-hops` and save them to `../feats/`.
The script supports multi-processing.  It is *recommended* to run several copies to speed up this step.
```
python tools/extract_pool5.py --gpu $GPU_ID &
```
For full argument configurations:
```
python tools/extract_pool5.py --fc $res50_OR_inception --model_path $CNN_MODEL_PATH --image_file $IMAGE_LIST_PATH --image_dir $DIR_TO_IMAGE --feat_dir $DIR_TO_SAVE_FEAT --gpu $GPU_ID &
```

## Testing
Although it takes 2 more steps to train GCN, at this point, we are ready to perform zero-shot classification with the [model](https://www.dropbox.com/sh/q9mid4wjj5vy0si/AADg8_NobfxkDot3VM7tE8Fua?dl=0) we provide.
```
python test_imagenet.py --model $MODEL_PATH
```
The above line defaults to `res50` + `2-hops` combination and test under two settings: unseen classes with or without seen classes. (see the paper for further explaination.)

We also provide other configurations. Please refer to the code for details.

## Train your own GCN model
### Get data ready for GCN training
1. Extract semantic embedding
- You can download the embedding from [here](https://www.dropbox.com/sh/9pklcwm7rkhd9qa/AACDMMKHIMXNW5cmInFFrCDCa?dl=0) and link to to `../data/word_embedding_model/`
- We also provide the script `tools/obtain_word_embedding.py` to obtain 3 version of semantic embeddings.
    + **Note**:
        *  GoogleNews and Fasttext embedding may require some other packages.
        * You need to download the pretrained word embedding dictionary. The link is written in the beginning of the script. Please manually download what you need since some model in the google drive cannot wget.
2.  Convert to GCN data. The script converts semantic embedding to input $X$; prepare class relation graph; and convert classifier of pretrained CNN model to output $W$.
The output will be saved to ../data/$wv_$fc/
```
python convert_to_gcn_data.py --fc res50 --wv glove
```
### Finally, start training!
```
python gcn/train_gcn.py --gpu $GPU_ID 	--dataset ../data/glove_res50/ --save_path $SAVE_PATH
```

## Citation
If you find this work helpful, please consider citing:
```
@article{wang2018zero,
  title={Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs},
  author={Wang, Xiaolong and Ye, Yufei and Gupta, Abhinav},
  journal={arXiv preprint arXiv:1803.08035},
  year={2018}
}
```



